{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "346776a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Hybrid Chunking Implementation (Codespaces Ready)\n",
    "\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "from typing import List, Dict, Tuple\n",
    "import json\n",
    "from collections import Counter\n",
    "import pickle\n",
    "from pathlib import Path\n",
    "import os\n",
    "\n",
    "# -------------------------------------------------------------------\n",
    "# Utility: Auto-detect dataset path (works inside /notebooks folder)\n",
    "# -------------------------------------------------------------------\n",
    "def get_dataset_path(filename=\"wikipedia_knowledge_base_balanced.csv\"):\n",
    "    \"\"\"Return dataset path automatically whether in /data/raw, /data, or repo root.\"\"\"\n",
    "    base_paths = [Path.cwd(), Path.cwd().parent]\n",
    "    possible_dirs = [\"data/raw\", \"data\", \".\"]\n",
    "    possible_paths = [b / d / filename for b in base_paths for d in possible_dirs]\n",
    "\n",
    "    for p in possible_paths:\n",
    "        if p.exists():\n",
    "            print(f\" Dataset found: {p.resolve()}\")\n",
    "            return str(p)\n",
    "    raise FileNotFoundError(f\" Dataset '{filename}' not found. Checked: {[str(p) for p in possible_paths]}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "80094247",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Step 2: Hybrid Chunking Implementation (Codespaces)\n",
      "======================================================================\n",
      "üìÇ Dataset found: /workspaces/Rag-Knowledge-Assiatant/data/wikipedia_knowledge_base_balanced.csv\n",
      " Loaded 150 articles from /workspaces/Rag-Knowledge-Assiatant/data/wikipedia_knowledge_base_balanced.csv\n",
      "üöÄ Processing Knowledge Base with Hybrid Chunking\n",
      "======================================================================\n",
      " [1/150] Physics\n",
      " [2/150] Chemistry\n",
      " [3/150] Biology\n",
      " [4/150] Quantum mechanics\n",
      " [5/150] Thermodynamics\n",
      " [6/150] Organic chemistry\n",
      " [7/150] Molecular biology\n",
      " [8/150] Genetics\n",
      " [9/150] Evolution\n",
      " [10/150] Mechanical engineering\n",
      " [11/150] Electrical engineering\n",
      " [12/150] Materials science\n",
      " [13/150] Artificial intelligence\n",
      " [14/150] Machine learning\n",
      " [15/150] Deep learning\n",
      " [16/150] Computer science\n",
      " [17/150] Software engineering\n",
      " [18/150] Data science\n",
      " [19/150] Computer security\n",
      " [20/150] Blockchain\n",
      " [21/150] Python (programming language)\n",
      " [22/150] Cloud computing\n",
      " [23/150] Quantum computing\n",
      " [24/150] Robotics\n",
      " [25/150] Mathematics\n",
      " [26/150] Calculus\n",
      " [27/150] Linear algebra\n",
      " [28/150] Statistics\n",
      " [29/150] Probability\n",
      " [30/150] Number theory\n",
      " [31/150] Geometry\n",
      " [32/150] Abstract algebra\n",
      " [33/150] Graph theory\n",
      " [34/150] Mathematical optimization\n",
      " [35/150] Bayesian statistics\n",
      " [36/150] Machine learning\n",
      " [37/150] World War II\n",
      " [38/150] World War I\n",
      " [39/150] Ancient Rome\n",
      " [40/150] Renaissance\n",
      " [41/150] Industrial Revolution\n",
      " [42/150] Geography\n",
      " [43/150] Climate change\n",
      " [44/150] Geology\n",
      " [45/150] Plate tectonics\n",
      " [46/150] Archaeology\n",
      " [47/150] Civilization\n",
      " [48/150] Democracy\n",
      " [49/150] Medicine\n",
      " [50/150] Anatomy\n",
      " [51/150] Physiology\n",
      " [52/150] Immunology\n",
      " [53/150] Cardiology\n",
      " [54/150] Neurology\n",
      " [55/150] Cancer\n",
      " [56/150] Diabetes\n",
      " [57/150] Vaccine\n",
      " [58/150] Antibiotic\n",
      " [59/150] Mental health\n",
      " [60/150] Public health\n",
      " [61/150] Mathematics\n",
      " [62/150] Calculus\n",
      " [63/150] Linear algebra\n",
      " [64/150] Statistics\n",
      " [65/150] Probability\n",
      " [66/150] Number theory\n",
      " [67/150] Geometry\n",
      " [68/150] Abstract algebra\n",
      " [69/150] Graph theory\n",
      " [70/150] Mathematical optimization\n",
      " [71/150] Bayesian statistics\n",
      " [72/150] Differential equation\n",
      " [73/150] Complex analysis\n",
      " [74/150] Real analysis\n",
      " [75/150] Discrete mathematics\n",
      " [76/150] Combinatorics\n",
      " [77/150] Medicine\n",
      " [78/150] Anatomy\n",
      " [79/150] Physiology\n",
      " [80/150] Immunology\n",
      " [81/150] Cardiology\n",
      " [82/150] Neurology\n",
      " [83/150] Cancer\n",
      " [84/150] Diabetes\n",
      " [85/150] Vaccine\n",
      " [86/150] Antibiotic\n",
      " [87/150] Mental health\n",
      " [88/150] Public health\n",
      " [89/150] Pharmacology\n",
      " [90/150] Surgery\n",
      " [91/150] Pathology\n",
      " [92/150] Oncology\n",
      " [93/150] Psychiatry\n",
      " [94/150] Epidemiology\n",
      " [95/150] World War II\n",
      " [96/150] World War I\n",
      " [97/150] Ancient Rome\n",
      " [98/150] Renaissance\n",
      " [99/150] Industrial Revolution\n",
      " [100/150] Geography\n",
      " [101/150] Climate change\n",
      " [102/150] Geology\n",
      " [103/150] Plate tectonics\n",
      " [104/150] Archaeology\n",
      " [105/150] Civilization\n",
      " [106/150] Democracy\n",
      " [107/150] French Revolution\n",
      " [108/150] American Civil War\n",
      " [109/150] Cold War\n",
      " [110/150] Ancient Greece\n",
      " [111/150] Industrial Revolution\n",
      " [112/150] Cold War\n",
      " [113/150] Linear algebra\n",
      " [114/150] Probability theory\n",
      " [115/150] Quantum mechanics\n",
      " [116/150] Thermodynamics\n",
      " [117/150] Organic chemistry\n",
      " [118/150] Mechanical engineering\n",
      " [119/150] Electrical engineering\n",
      " [120/150] Civil engineering\n",
      " [121/150] Chemical engineering\n",
      " [122/150] Aerospace engineering\n",
      " [123/150] Materials science\n",
      " [124/150] Environmental engineering\n",
      " [125/150] Biomedical engineering\n",
      " [126/150] Nuclear engineering\n",
      " [127/150] Fluid mechanics\n",
      " [128/150] Electromagnetism\n",
      " [129/150] Optics\n",
      " [130/150] Acoustics\n",
      " [131/150] Crystallography\n",
      " [132/150] Polymer science\n",
      " [133/150] Artificial intelligence\n",
      " [134/150] Machine learning\n",
      " [135/150] Deep learning\n",
      " [136/150] Computer network\n",
      " [137/150] Database\n",
      " [138/150] Operating system\n",
      " [139/150] Software engineering\n",
      " [140/150] Computer security\n",
      " [141/150] Cloud computing\n",
      " [142/150] Data structure\n",
      " [143/150] Algorithm\n",
      " [144/150] Programming language\n",
      " [145/150] Computer graphics\n",
      " [146/150] Human‚Äìcomputer interaction\n",
      " [147/150] Distributed computing\n",
      " [148/150] Quantum computing\n",
      " [149/150] Blockchain\n",
      " [150/150] Internet of things\n",
      "\n",
      " Hybrid Chunking Results Summary\n",
      "============================================================\n",
      " Total articles: 150\n",
      " Total chunks: 2979\n",
      "üìè Avg chunks/article: 19.9\n",
      "\n",
      "üéØ Chunk Type Distribution:\n",
      "   title_beginning: 150\n",
      "   definitions: 87\n",
      "   content: 2742\n",
      "\n",
      " Priority Distribution:\n",
      "   HIGH: 237\n",
      "   MEDIUM: 2742\n",
      " Saved hybrid chunks ‚Üí /workspaces/Rag-Knowledge-Assiatant/notebooks/data/processed/hybrid_chunks.pkl\n",
      "\n",
      " Step 2 Complete! 2979 chunks created.\n",
      " File saved: data/processed/hybrid_chunks.pkl\n",
      " Ready for Step 3: Embedding Creation\n"
     ]
    }
   ],
   "source": [
    "# -------------------------------------------------------------------\n",
    "# Hybrid Chunker Class\n",
    "# -------------------------------------------------------------------\n",
    "class HybridChunker:\n",
    "    \"\"\"Production-ready hybrid chunking implementation (Codespaces version).\"\"\"\n",
    "\n",
    "    def __init__(self):\n",
    "        self.chunks = []\n",
    "        self.article_stats = {}\n",
    "\n",
    "    # -----------------------------\n",
    "    # Main chunking logic\n",
    "    # -----------------------------\n",
    "    def create_hybrid_chunks(self, article_row: pd.Series) -> List[Dict]:\n",
    "        title = article_row[\"title\"]\n",
    "        content = article_row[\"content\"]\n",
    "        domain = article_row[\"domain\"]\n",
    "        url = article_row.get(\"url\", \"\")\n",
    "\n",
    "        article_chunks = []\n",
    "        article_chunks.append(self._create_title_beginning_chunk(title, content, domain, url))\n",
    "\n",
    "        definition_chunk = self._create_definitions_chunk(title, content, domain)\n",
    "        if definition_chunk:\n",
    "            article_chunks.append(definition_chunk)\n",
    "\n",
    "        content_chunks = self._create_content_chunks(title, content, domain, url)\n",
    "        article_chunks.extend(content_chunks)\n",
    "\n",
    "        return article_chunks\n",
    "\n",
    "    # --- title + intro chunk ---\n",
    "    def _create_title_beginning_chunk(self, title: str, content: str, domain: str, url: str) -> Dict:\n",
    "        beginning_text = content[:1000]\n",
    "        sentences = re.split(r\"(?<=[.!?])\\s+\", beginning_text)\n",
    "        if len(sentences) > 1:\n",
    "            beginning_text = \". \".join(sentences[:-1]) + \".\"\n",
    "\n",
    "        chunk_text = f\"Title: {title}\\nDomain: {domain}\\n\\n{beginning_text}\"\n",
    "\n",
    "        return {\n",
    "            \"text\": chunk_text,\n",
    "            \"chunk_type\": \"title_beginning\",\n",
    "            \"priority\": \"HIGH\",\n",
    "            \"metadata\": {\n",
    "                \"title\": title,\n",
    "                \"domain\": domain,\n",
    "                \"url\": url,\n",
    "                \"word_count\": len(chunk_text.split()),\n",
    "                \"char_count\": len(chunk_text),\n",
    "            },\n",
    "        }\n",
    "\n",
    "    # --- definitions chunk ---\n",
    "    def _create_definitions_chunk(self, title: str, content: str, domain: str) -> Dict:\n",
    "        first_paragraph = content.split(\"\\n\\n\")[0] if \"\\n\\n\" in content else content[:800]\n",
    "        patterns = [\n",
    "            rf\"{re.escape(title)} is (?:the|a|an) (.+?)(?:\\.|,|;)\",\n",
    "            rf\"{re.escape(title)} refers to (.+?)(?:\\.|,|;)\",\n",
    "            rf\"{re.escape(title)} (?:means|denotes|represents) (.+?)(?:\\.|,|;)\",\n",
    "        ]\n",
    "\n",
    "        definitions = []\n",
    "        for pattern in patterns:\n",
    "            matches = re.findall(pattern, first_paragraph, re.IGNORECASE | re.DOTALL)\n",
    "            for m in matches:\n",
    "                definitions.append((title, m.strip()))\n",
    "\n",
    "        if not definitions:\n",
    "            return None\n",
    "\n",
    "        def_text = f\"Key definitions for {title} ({domain}):\\n\\n\"\n",
    "        for term, definition in definitions[:3]:\n",
    "            def_text += f\"‚Ä¢ {term}: {definition}\\n\"\n",
    "\n",
    "        return {\n",
    "            \"text\": def_text,\n",
    "            \"chunk_type\": \"definitions\",\n",
    "            \"priority\": \"HIGH\",\n",
    "            \"metadata\": {\n",
    "                \"title\": title,\n",
    "                \"domain\": domain,\n",
    "                \"definitions_count\": len(definitions),\n",
    "                \"word_count\": len(def_text.split()),\n",
    "                \"char_count\": len(def_text),\n",
    "            },\n",
    "        }\n",
    "\n",
    "    # --- content chunks ---\n",
    "    def _create_content_chunks(self, title: str, content: str, domain: str, url: str) -> List[Dict]:\n",
    "        sentences = re.split(r\"(?<=[.!?])\\s+\", content)\n",
    "        sentences = [s.strip() for s in sentences if len(s.strip()) > 20]\n",
    "\n",
    "        chunks, current_chunk, current_wc = [], [], 0\n",
    "        target_size, overlap = 400, 2\n",
    "\n",
    "        for i, sentence in enumerate(sentences):\n",
    "            sw = len(sentence.split())\n",
    "            if current_wc + sw > target_size and current_chunk:\n",
    "                chunk_text = \" \".join(current_chunk)\n",
    "                chunks.append({\n",
    "                    \"text\": chunk_text,\n",
    "                    \"chunk_type\": \"content\",\n",
    "                    \"priority\": \"MEDIUM\",\n",
    "                    \"metadata\": {\n",
    "                        \"title\": title,\n",
    "                        \"domain\": domain,\n",
    "                        \"url\": url,\n",
    "                        \"sentence_start\": i - len(current_chunk),\n",
    "                        \"sentence_end\": i,\n",
    "                        \"word_count\": current_wc,\n",
    "                        \"char_count\": len(chunk_text),\n",
    "                    },\n",
    "                })\n",
    "                # overlap\n",
    "                current_chunk = current_chunk[-overlap:]\n",
    "                current_wc = sum(len(s.split()) for s in current_chunk)\n",
    "            current_chunk.append(sentence)\n",
    "            current_wc += sw\n",
    "\n",
    "        if current_chunk and current_wc > 50:\n",
    "            chunk_text = \" \".join(current_chunk)\n",
    "            chunks.append({\n",
    "                \"text\": chunk_text,\n",
    "                \"chunk_type\": \"content\",\n",
    "                \"priority\": \"MEDIUM\",\n",
    "                \"metadata\": {\n",
    "                    \"title\": title,\n",
    "                    \"domain\": domain,\n",
    "                    \"url\": url,\n",
    "                    \"sentence_start\": len(sentences) - len(current_chunk),\n",
    "                    \"sentence_end\": len(sentences),\n",
    "                    \"word_count\": current_wc,\n",
    "                    \"char_count\": len(chunk_text),\n",
    "                },\n",
    "            })\n",
    "        return chunks\n",
    "\n",
    "    # -----------------------------\n",
    "    # Process all articles\n",
    "    # -----------------------------\n",
    "    def process_knowledge_base(self, df: pd.DataFrame) -> Dict:\n",
    "        print(\"üöÄ Processing Knowledge Base with Hybrid Chunking\")\n",
    "        print(\"=\" * 70)\n",
    "\n",
    "        all_chunks = []\n",
    "        stats = {\n",
    "            \"total_articles\": len(df),\n",
    "            \"total_chunks\": 0,\n",
    "            \"chunk_types\": Counter(),\n",
    "            \"priority_distribution\": Counter(),\n",
    "            \"domain_stats\": {},\n",
    "        }\n",
    "\n",
    "        for idx, article in df.iterrows():\n",
    "            print(f\" [{idx+1}/{len(df)}] {article['title']}\")\n",
    "            article_chunks = self.create_hybrid_chunks(article)\n",
    "\n",
    "            for chunk in article_chunks:\n",
    "                chunk[\"article_idx\"] = idx\n",
    "                chunk[\"chunk_id\"] = len(all_chunks)\n",
    "                all_chunks.append(chunk)\n",
    "                stats[\"chunk_types\"][chunk[\"chunk_type\"]] += 1\n",
    "                stats[\"priority_distribution\"][chunk[\"priority\"]] += 1\n",
    "\n",
    "            domain = article[\"domain\"]\n",
    "            if domain not in stats[\"domain_stats\"]:\n",
    "                stats[\"domain_stats\"][domain] = {\"articles\": 0, \"chunks\": 0}\n",
    "            stats[\"domain_stats\"][domain][\"articles\"] += 1\n",
    "            stats[\"domain_stats\"][domain][\"chunks\"] += len(article_chunks)\n",
    "\n",
    "        stats[\"total_chunks\"] = len(all_chunks)\n",
    "        self.chunks = all_chunks\n",
    "        self.article_stats = stats\n",
    "        return {\"chunks\": all_chunks, \"stats\": stats}\n",
    "    \n",
    "    # -----------------------------\n",
    "    # Analysis & save utilities\n",
    "    # -----------------------------\n",
    "    def analyze_chunking_results(self):\n",
    "        s = self.article_stats\n",
    "        print(f\"\\n Hybrid Chunking Results Summary\")\n",
    "        print(\"=\" * 60)\n",
    "        print(f\" Total articles: {s['total_articles']}\")\n",
    "        print(f\" Total chunks: {s['total_chunks']}\")\n",
    "        print(f\"üìè Avg chunks/article: {s['total_chunks']/s['total_articles']:.1f}\")\n",
    "        print(\"\\nüéØ Chunk Type Distribution:\")\n",
    "        for t, c in s[\"chunk_types\"].items():\n",
    "            print(f\"   {t}: {c}\")\n",
    "        print(\"\\n Priority Distribution:\")\n",
    "        for p, c in s[\"priority_distribution\"].items():\n",
    "            print(f\"   {p}: {c}\")\n",
    "\n",
    "    def save_chunks(self, out_path=\"data/processed/hybrid_chunks.pkl\"):\n",
    "        out_file = Path(out_path)\n",
    "        out_file.parent.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "        data = {\n",
    "            \"chunks\": self.chunks,\n",
    "            \"stats\": self.article_stats,\n",
    "            \"method\": \"hybrid\",\n",
    "        }\n",
    "        with open(out_file, \"wb\") as f:\n",
    "            pickle.dump(data, f)\n",
    "        print(f\" Saved hybrid chunks ‚Üí {out_file.resolve()}\")\n",
    "        return str(out_file)\n",
    "\n",
    "# -------------------------------------------------------------------\n",
    "# Runner Function (Codespaces)\n",
    "# -------------------------------------------------------------------\n",
    "def run_hybrid_chunking():\n",
    "    print(\" Step 2: Hybrid Chunking Implementation (Codespaces)\")\n",
    "    print(\"=\" * 70)\n",
    "\n",
    "    csv_path = get_dataset_path()\n",
    "    df = pd.read_csv(csv_path)\n",
    "    print(f\" Loaded {len(df)} articles from {csv_path}\")\n",
    "\n",
    "    chunker = HybridChunker()\n",
    "    results = chunker.process_knowledge_base(df)\n",
    "    chunker.analyze_chunking_results()\n",
    "\n",
    "    saved_file = chunker.save_chunks()\n",
    "    print(f\"\\n Step 2 Complete! {len(results['chunks'])} chunks created.\")\n",
    "    print(f\" File saved: {saved_file}\")\n",
    "    print(\" Ready for Step 3: Embedding Creation\")\n",
    "\n",
    "    return results, chunker\n",
    "\n",
    "# Execute directly\n",
    "if __name__ == \"__main__\":\n",
    "    results, chunker = run_hybrid_chunking()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
