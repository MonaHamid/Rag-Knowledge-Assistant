{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "2a7305f8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🚀 STEP 4.5: COMPREHENSIVE EVALUATION FRAMEWORK (Codespaces Ready)\n",
      "======================================================================\n",
      "📂 Found retrieval system under: /workspaces/Rag-Knowledge-Assiatant/notebooks/data/processed\n",
      "📂 Loading retrieval system for evaluation from data/processed/retrieval_system ...\n",
      "🤖 Loading embedding model: all-MiniLM-L6-v2\n",
      "✅ Loaded 3022 chunks | 384D embeddings\n",
      "🚀 COMPREHENSIVE RETRIEVAL EVALUATION\n",
      "======================================================================\n",
      "📋 Creating evaluation dataset...\n",
      "✅ Created evaluation dataset with 15 queries\n",
      "   🎯 Query types: {'definition': 10, 'process': 3, 'explanation': 1, 'concept': 1}\n",
      "   📈 Difficulties: {'easy': 3, 'medium': 7, 'hard': 5}\n",
      "\n",
      "📊 EVALUATING RETRIEVAL METHODS\n",
      "--------------------------------------------------\n",
      "🔍 Evaluating BM25 retrieval...\n",
      "✅ BM25 evaluation complete\n",
      "🔍 Evaluating dense retrieval...\n",
      "✅ Dense retrieval evaluation complete\n",
      "🔍 Evaluating hybrid retrieval...\n",
      "✅ Hybrid retrieval evaluation complete\n",
      "📊 Calculating metrics for BM25...\n",
      "✅ BM25 metrics calculated\n",
      "📊 Calculating metrics for Dense...\n",
      "✅ Dense metrics calculated\n",
      "📊 Calculating metrics for Hybrid...\n",
      "✅ Hybrid metrics calculated\n",
      "\n",
      "📊 RETRIEVAL EVALUATION RESULTS\n",
      "======================================================================\n",
      "Method       Precision@5  Recall@5   MRR      HighPriority  BeginContent \n",
      "---------------------------------------------------------------------------\n",
      "BM25         0.17        0.87      0.82     0.25          0.87\n",
      "Dense        0.20        0.93      0.93     0.28          1.00\n",
      "Hybrid       0.23        1.00      0.90     0.65          1.00\n",
      "\n",
      "🏆 Best overall method: DENSE\n",
      "💾 Evaluation results saved to evaluation_results.json\n",
      "\n",
      "✅ STEP 4.5 COMPLETE!\n",
      "📋 Evaluated BM25, Dense & Hybrid retrieval methods\n",
      "📊 Results stored in evaluation_results.json\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Evaluates BM25, Dense, and Hybrid retrieval methods using precision,\n",
    "# recall, MRR, and coverage metrics on a standard query dataset.\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import pickle\n",
    "import faiss\n",
    "from sentence_transformers import SentenceTransformer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "import time\n",
    "import re\n",
    "from typing import List, Dict, Tuple, Optional\n",
    "from collections import Counter\n",
    "import json\n",
    "from pathlib import Path\n",
    "\n",
    "# ================================================================\n",
    "# Comprehensive Evaluator Class\n",
    "# ================================================================\n",
    "class ComprehensiveEvaluator:\n",
    "    \"\"\"Comprehensive evaluation framework for retrieval and answer generation\"\"\"\n",
    "\n",
    "    def __init__(self):\n",
    "        self.chunks = []\n",
    "        self.embeddings = None\n",
    "        self.model = None\n",
    "        self.evaluation_queries = []\n",
    "        self.ground_truth = {}\n",
    "        self.retrieval_results = {}\n",
    "        self.evaluation_metrics = {}\n",
    "\n",
    "    # ------------------------------------------------------------\n",
    "    # 🔍 Codespaces path auto-detection\n",
    "    # ------------------------------------------------------------\n",
    "    def _detect_data_path(self) -> Path:\n",
    "        possible_dirs = [\n",
    "            Path(\"notebooks/data/processed\"),\n",
    "            Path(\"data/processed\"),\n",
    "            Path(\"processed\"),\n",
    "            Path(\"data\")\n",
    "        ]\n",
    "        for d in possible_dirs:\n",
    "            if d.exists() and any(\"retrieval_system\" in str(p) for p in d.glob(\"*\")):\n",
    "                print(f\"📂 Found retrieval system under: {d.resolve()}\")\n",
    "                return d\n",
    "        print(\"⚠️ Retrieval system directory not found — defaulting to current path.\")\n",
    "        return Path(\".\")\n",
    "\n",
    "    # ------------------------------------------------------------\n",
    "    # 📦 Load retrieval system (embeddings + metadata)\n",
    "    # ------------------------------------------------------------\n",
    "    def load_retrieval_system(self, prefix: str = 'retrieval_system'):\n",
    "        data_dir = self._detect_data_path()\n",
    "        prefix_path = data_dir / prefix\n",
    "        print(f\"📂 Loading retrieval system for evaluation from {prefix_path} ...\")\n",
    "\n",
    "        try:\n",
    "            self.embeddings = np.load(f\"{prefix_path}_embeddings.npy\")\n",
    "            with open(f\"{prefix_path}_metadata.pkl\", 'rb') as f:\n",
    "                metadata = pickle.load(f)\n",
    "\n",
    "            self.chunks = metadata['chunks']\n",
    "            model_name = metadata['model_name']\n",
    "\n",
    "            if model_name != 'tfidf_fallback':\n",
    "                print(f\"🤖 Loading embedding model: {model_name}\")\n",
    "                self.model = SentenceTransformer(model_name)\n",
    "\n",
    "            print(f\"✅ Loaded {len(self.chunks)} chunks | {self.embeddings.shape[1]}D embeddings\")\n",
    "            return True\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"❌ Error loading system: {e}\")\n",
    "            return False\n",
    "\n",
    "    # ------------------------------------------------------------\n",
    "    # 🧩 Create evaluation dataset\n",
    "    # ------------------------------------------------------------\n",
    "    def create_evaluation_dataset(self) -> Dict:\n",
    "        print(f\"📋 Creating evaluation dataset...\")\n",
    "\n",
    "        evaluation_data = {\n",
    "            \"What is physics?\": {\"expected_articles\": [\"Physics\"],\n",
    "                \"expected_chunk_types\": [\"title_beginning\", \"definitions\"],\n",
    "                \"query_type\": \"definition\", \"difficulty\": \"easy\"},\n",
    "            \"Define artificial intelligence\": {\"expected_articles\": [\"Artificial intelligence\"],\n",
    "                \"expected_chunk_types\": [\"definitions\", \"title_beginning\"],\n",
    "                \"query_type\": \"definition\", \"difficulty\": \"easy\"},\n",
    "            \"What is quantum mechanics?\": {\"expected_articles\": [\"Quantum mechanics\"],\n",
    "                \"expected_chunk_types\": [\"title_beginning\", \"definitions\"],\n",
    "                \"query_type\": \"definition\", \"difficulty\": \"medium\"},\n",
    "            \"Define machine learning\": {\"expected_articles\": [\"Machine learning\"],\n",
    "                \"expected_chunk_types\": [\"definitions\", \"title_beginning\"],\n",
    "                \"query_type\": \"definition\", \"difficulty\": \"medium\"},\n",
    "            \"What is calculus?\": {\"expected_articles\": [\"Calculus\"],\n",
    "                \"expected_chunk_types\": [\"title_beginning\", \"definitions\"],\n",
    "                \"query_type\": \"definition\", \"difficulty\": \"easy\"},\n",
    "            \"How does photosynthesis work?\": {\"expected_articles\": [\"Biology\", \"Photosynthesis\"],\n",
    "                \"expected_chunk_types\": [\"content\"], \"query_type\": \"process\", \"difficulty\": \"hard\"},\n",
    "            \"How does machine learning work?\": {\"expected_articles\": [\"Machine learning\"],\n",
    "                \"expected_chunk_types\": [\"content\", \"definitions\"],\n",
    "                \"query_type\": \"process\", \"difficulty\": \"medium\"},\n",
    "            \"Explain DNA structure\": {\"expected_articles\": [\"Biology\", \"Genetics\", \"DNA\"],\n",
    "                \"expected_chunk_types\": [\"content\"],\n",
    "                \"query_type\": \"explanation\", \"difficulty\": \"hard\"},\n",
    "            \"What is the theory of relativity?\": {\"expected_articles\": [\"Physics\", \"Theory of relativity\", \"Einstein\"],\n",
    "                \"expected_chunk_types\": [\"content\", \"definitions\"],\n",
    "                \"query_type\": \"concept\", \"difficulty\": \"hard\"},\n",
    "            \"What is climate change?\": {\"expected_articles\": [\"Climate change\"],\n",
    "                \"expected_chunk_types\": [\"title_beginning\", \"definitions\"],\n",
    "                \"query_type\": \"definition\", \"difficulty\": \"medium\"},\n",
    "            \"What is organic chemistry?\": {\"expected_articles\": [\"Chemistry\", \"Organic chemistry\"],\n",
    "                \"expected_chunk_types\": [\"definitions\", \"title_beginning\"],\n",
    "                \"query_type\": \"definition\", \"difficulty\": \"medium\"},\n",
    "            \"Define neural networks\": {\"expected_articles\": [\"Machine learning\", \"Artificial intelligence\", \"Neural networks\"],\n",
    "                \"expected_chunk_types\": [\"content\", \"definitions\"],\n",
    "                \"query_type\": \"definition\", \"difficulty\": \"hard\"},\n",
    "            \"What is bioinformatics?\": {\"expected_articles\": [\"Biology\", \"Computer science\", \"Bioinformatics\"],\n",
    "                \"expected_chunk_types\": [\"content\", \"definitions\"],\n",
    "                \"query_type\": \"definition\", \"difficulty\": \"hard\"},\n",
    "            \"How does evolution work?\": {\"expected_articles\": [\"Biology\", \"Evolution\"],\n",
    "                \"expected_chunk_types\": [\"content\", \"title_beginning\"],\n",
    "                \"query_type\": \"process\", \"difficulty\": \"medium\"},\n",
    "            \"What is linear algebra?\": {\"expected_articles\": [\"Mathematics\", \"Linear algebra\"],\n",
    "                \"expected_chunk_types\": [\"definitions\", \"title_beginning\"],\n",
    "                \"query_type\": \"definition\", \"difficulty\": \"medium\"}\n",
    "        }\n",
    "\n",
    "        self.evaluation_queries = list(evaluation_data.keys())\n",
    "        self.ground_truth = evaluation_data\n",
    "\n",
    "        print(f\"✅ Created evaluation dataset with {len(self.evaluation_queries)} queries\")\n",
    "        query_types = Counter(d['query_type'] for d in evaluation_data.values())\n",
    "        difficulties = Counter(d['difficulty'] for d in evaluation_data.values())\n",
    "        print(f\"   🎯 Query types: {dict(query_types)}\")\n",
    "        print(f\"   📈 Difficulties: {dict(difficulties)}\")\n",
    "        return evaluation_data\n",
    "\n",
    "    # ------------------------------------------------------------\n",
    "    # 🔹 Evaluate BM25 (TF-IDF cosine)\n",
    "    # ------------------------------------------------------------\n",
    "    def evaluate_bm25_retrieval(self, k: int = 5) -> Dict:\n",
    "        print(f\"🔍 Evaluating BM25 retrieval...\")\n",
    "        chunk_texts = [chunk['text'] for chunk in self.chunks]\n",
    "        vectorizer = TfidfVectorizer(\n",
    "            max_features=5000, stop_words='english',\n",
    "            ngram_range=(1, 2), min_df=2, max_df=0.8)\n",
    "        tfidf_matrix = vectorizer.fit_transform(chunk_texts)\n",
    "        results = {}\n",
    "        for query in self.evaluation_queries:\n",
    "            qv = vectorizer.transform([query])\n",
    "            sims = cosine_similarity(qv, tfidf_matrix).flatten()\n",
    "            top_idx = np.argsort(sims)[::-1][:k]\n",
    "            res = []\n",
    "            for idx in top_idx:\n",
    "                chunk = self.chunks[idx]\n",
    "                res.append({\n",
    "                    'chunk_idx': int(idx), 'score': float(sims[idx]),\n",
    "                    'title': chunk['metadata']['title'],\n",
    "                    'chunk_type': chunk['chunk_type'],\n",
    "                    'priority': chunk['priority'], 'text': chunk['text']\n",
    "                })\n",
    "            results[query] = res\n",
    "        print(f\"✅ BM25 evaluation complete\")\n",
    "        return results\n",
    "\n",
    "    # ------------------------------------------------------------\n",
    "    # 🔹 Evaluate dense retrieval (Sentence Transformers)\n",
    "    # ------------------------------------------------------------\n",
    "    def evaluate_dense_retrieval(self, k: int = 5) -> Dict:\n",
    "        print(f\"🔍 Evaluating dense retrieval...\")\n",
    "        if self.model is None:\n",
    "            print(\"❌ No embedding model available\")\n",
    "            return {}\n",
    "        index = faiss.IndexFlatIP(self.embeddings.shape[1])\n",
    "        embs = self.embeddings.copy()\n",
    "        faiss.normalize_L2(embs)\n",
    "        index.add(embs.astype('float32'))\n",
    "        results = {}\n",
    "        for query in self.evaluation_queries:\n",
    "            q_emb = self.model.encode([query])\n",
    "            faiss.normalize_L2(q_emb.astype('float32'))\n",
    "            scores, idxs = index.search(q_emb.astype('float32'), k)\n",
    "            res = []\n",
    "            for i, s in zip(idxs[0], scores[0]):\n",
    "                if i < len(self.chunks):\n",
    "                    c = self.chunks[i]\n",
    "                    res.append({'chunk_idx': int(i), 'score': float(s),\n",
    "                                'title': c['metadata']['title'],\n",
    "                                'chunk_type': c['chunk_type'],\n",
    "                                'priority': c['priority'], 'text': c['text']})\n",
    "            results[query] = res\n",
    "        print(f\"✅ Dense retrieval evaluation complete\")\n",
    "        return results\n",
    "\n",
    "    # ------------------------------------------------------------\n",
    "    # 🔹 Evaluate hybrid retrieval\n",
    "    # ------------------------------------------------------------\n",
    "    def evaluate_hybrid_retrieval(self, k: int = 5) -> Dict:\n",
    "        print(f\"🔍 Evaluating hybrid retrieval...\")\n",
    "        pri_w = {'HIGH': 1.5, 'MEDIUM': 1.0}\n",
    "        type_w = {'title_beginning': 1.3, 'definitions': 1.4, 'content': 1.0}\n",
    "        index = faiss.IndexFlatIP(self.embeddings.shape[1])\n",
    "        embs = self.embeddings.copy()\n",
    "        faiss.normalize_L2(embs)\n",
    "        index.add(embs.astype('float32'))\n",
    "        results = {}\n",
    "        for query in self.evaluation_queries:\n",
    "            q_emb = self.model.encode([query])\n",
    "            faiss.normalize_L2(q_emb.astype('float32'))\n",
    "            init_k = min(k * 3, len(self.chunks))\n",
    "            scores, idxs = index.search(q_emb.astype('float32'), init_k)\n",
    "            enhanced = []\n",
    "            for i, s in zip(idxs[0], scores[0]):\n",
    "                if i < len(self.chunks):\n",
    "                    c = self.chunks[i]\n",
    "                    pw = pri_w.get(c['priority'], 1.0)\n",
    "                    ql = query.lower()\n",
    "                    if any(p in ql for p in ['what is', 'define']):\n",
    "                        tw = 1.5 if c['chunk_type'] == 'definitions' else 1.3 if c['chunk_type'] == 'title_beginning' else 1.0\n",
    "                    else:\n",
    "                        tw = type_w.get(c['chunk_type'], 1.0)\n",
    "                    enhanced.append({'chunk_idx': int(i), 'score': float(s)*pw*tw,\n",
    "                                     'original_score': float(s),\n",
    "                                     'title': c['metadata']['title'],\n",
    "                                     'chunk_type': c['chunk_type'],\n",
    "                                     'priority': c['priority'], 'text': c['text']})\n",
    "            enhanced.sort(key=lambda x: x['score'], reverse=True)\n",
    "            results[query] = enhanced[:k]\n",
    "        print(f\"✅ Hybrid retrieval evaluation complete\")\n",
    "        return results\n",
    "\n",
    "    # ------------------------------------------------------------\n",
    "    # 📈 Compute metrics (Precision@5, Recall@5, MRR)\n",
    "    # ------------------------------------------------------------\n",
    "    def calculate_retrieval_metrics(self, results: Dict, method_name: str) -> Dict:\n",
    "        print(f\"📊 Calculating metrics for {method_name}...\")\n",
    "        m = {'precision_at_5': [], 'recall_at_5': [], 'mrr': [],\n",
    "             'high_priority_coverage': [], 'beginning_content_coverage': []}\n",
    "        detailed = []\n",
    "        for q, res in results.items():\n",
    "            if q not in self.ground_truth: continue\n",
    "            gt = self.ground_truth[q]\n",
    "            exp_articles = set(gt['expected_articles'])\n",
    "            exp_types = set(gt['expected_chunk_types'])\n",
    "            ret_articles = set(r['title'] for r in res)\n",
    "            ret_types = set(r['chunk_type'] for r in res)\n",
    "            pri = [r['priority'] for r in res]\n",
    "            rel = len(exp_articles & ret_articles)\n",
    "            prec = rel / min(5, len(res)) if res else 0\n",
    "            rec = min(1.0, rel) if exp_articles else 0\n",
    "            mrr = 0\n",
    "            for i, r in enumerate(res):\n",
    "                if r['title'] in exp_articles:\n",
    "                    mrr = 1.0 / (i + 1)\n",
    "                    break\n",
    "            high_cov = sum(p == 'HIGH' for p in pri) / len(res) if res else 0\n",
    "            begin_cov = 1.0 if exp_types & ret_types else 0.0\n",
    "            for key, val in zip(m.keys(), [prec, rec, mrr, high_cov, begin_cov]):\n",
    "                m[key].append(val)\n",
    "            detailed.append({'query': q, 'query_type': gt['query_type'],\n",
    "                             'difficulty': gt['difficulty'],\n",
    "                             'precision_at_5': prec, 'recall_at_5': rec,\n",
    "                             'mrr': mrr, 'high_priority_coverage': high_cov,\n",
    "                             'beginning_content_coverage': begin_cov,\n",
    "                             'expected_articles': list(exp_articles),\n",
    "                             'retrieved_articles': list(ret_articles),\n",
    "                             'relevant_found': list(exp_articles & ret_articles)})\n",
    "        avg = {k: np.mean(v) for k, v in m.items()}\n",
    "        print(f\"✅ {method_name} metrics calculated\")\n",
    "        return {'method': method_name, 'avg_metrics': avg,\n",
    "                'detailed_results': detailed, 'raw_metrics': m}\n",
    "\n",
    "    # ------------------------------------------------------------\n",
    "    # 🚀 Run comprehensive evaluation\n",
    "    # ------------------------------------------------------------\n",
    "    def run_comprehensive_evaluation(self) -> Dict:\n",
    "        print(f\"🚀 COMPREHENSIVE RETRIEVAL EVALUATION\")\n",
    "        print(\"=\" * 70)\n",
    "        self.create_evaluation_dataset()\n",
    "        print(f\"\\n📊 EVALUATING RETRIEVAL METHODS\")\n",
    "        print(\"-\" * 50)\n",
    "        bm25 = self.evaluate_bm25_retrieval()\n",
    "        dense = self.evaluate_dense_retrieval()\n",
    "        hybrid = self.evaluate_hybrid_retrieval()\n",
    "        bm25_m = self.calculate_retrieval_metrics(bm25, \"BM25\")\n",
    "        dense_m = self.calculate_retrieval_metrics(dense, \"Dense\")\n",
    "        hybrid_m = self.calculate_retrieval_metrics(hybrid, \"Hybrid\")\n",
    "        self.retrieval_results = {'bm25': bm25, 'dense': dense, 'hybrid': hybrid}\n",
    "        self.evaluation_metrics = {'bm25': bm25_m, 'dense': dense_m, 'hybrid': hybrid_m}\n",
    "        self.print_comparison_table()\n",
    "        return {'results': self.retrieval_results,\n",
    "                'metrics': self.evaluation_metrics,\n",
    "                'evaluation_dataset': self.ground_truth}\n",
    "\n",
    "    # ------------------------------------------------------------\n",
    "    # 🧾 Display comparison table\n",
    "    # ------------------------------------------------------------\n",
    "    def print_comparison_table(self):\n",
    "        print(f\"\\n📊 RETRIEVAL EVALUATION RESULTS\")\n",
    "        print(\"=\" * 70)\n",
    "        methods = ['BM25', 'Dense', 'Hybrid']\n",
    "        print(f\"{'Method':<12} {'Precision@5':<12} {'Recall@5':<10} {'MRR':<8} \"\n",
    "              f\"{'HighPriority':<13} {'BeginContent':<13}\")\n",
    "        print(\"-\" * 75)\n",
    "        for key, name in zip(['bm25', 'dense', 'hybrid'], methods):\n",
    "            a = self.evaluation_metrics[key]['avg_metrics']\n",
    "            print(f\"{name:<12} {a['precision_at_5']:.2f}        {a['recall_at_5']:.2f}      \"\n",
    "                  f\"{a['mrr']:.2f}     {a['high_priority_coverage']:.2f}          \"\n",
    "                  f\"{a['beginning_content_coverage']:.2f}\")\n",
    "        best = max(self.evaluation_metrics.keys(),\n",
    "                   key=lambda x: self.evaluation_metrics[x]['avg_metrics']['mrr'])\n",
    "        print(f\"\\n🏆 Best overall method: {best.upper()}\")\n",
    "\n",
    "    # ------------------------------------------------------------\n",
    "    # 💾 Save results to JSON\n",
    "    # ------------------------------------------------------------\n",
    "    def save_evaluation_results(self, filename: str = 'evaluation_results.json'):\n",
    "        res = {\n",
    "            'evaluation_dataset': self.ground_truth,\n",
    "            'metrics_summary': {m: d['avg_metrics'] for m, d in self.evaluation_metrics.items()},\n",
    "            'detailed_results': {m: d['detailed_results'] for m, d in self.evaluation_metrics.items()}\n",
    "        }\n",
    "        with open(filename, 'w') as f:\n",
    "            json.dump(res, f, indent=2)\n",
    "        print(f\"💾 Evaluation results saved to {filename}\")\n",
    "        return filename\n",
    "\n",
    "\n",
    "# ================================================================\n",
    "# Main entrypoint\n",
    "# ================================================================\n",
    "def run_comprehensive_evaluation():\n",
    "    print(\"🚀 STEP 4.5: COMPREHENSIVE EVALUATION FRAMEWORK (Codespaces Ready)\")\n",
    "    print(\"=\" * 70)\n",
    "    evaluator = ComprehensiveEvaluator()\n",
    "    if not evaluator.load_retrieval_system():\n",
    "        print(\"❌ Failed to load retrieval system\")\n",
    "        return None\n",
    "    results = evaluator.run_comprehensive_evaluation()\n",
    "    evaluator.save_evaluation_results()\n",
    "    print(f\"\\n✅ STEP 4.5 COMPLETE!\")\n",
    "    print(f\"📋 Evaluated BM25, Dense & Hybrid retrieval methods\")\n",
    "    print(f\"📊 Results stored in evaluation_results.json\")\n",
    "    return evaluator, results\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    evaluator, results = run_comprehensive_evaluation()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
